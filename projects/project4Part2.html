<!DOCTYPE HTML>
<html>
	<head>
		<title>e-Portfolio Elias - Projects Page</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
			<div id="wrapper">
				<header id="header">
					<div class="inner">
						<div class="button-container">
							<a href="../index.html" class="logo home-button">
								<span class="symbol"><img src="../images/home-icon.png" alt="" /></span><span class="title">Home</span>
							</a>
							<a href="../projects.html" class="logo back-button">
								<span class="symbol"><img src="../images/back-icon.png" alt="" /></span><span class="title">Back</span>
							</a>
						</div>
							<nav>
								<ul>
									<li><a href="#menu">Menu</a></li>
								</ul>
							</nav>
					</div>
				</header>
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<button id="darkModeToggle">Toggle Dark Mode</button>
						<br>
						<br>
						<li><a href="../index.html">Home</a></li>
						<li><a href="../modulePages/inductionModule.html">Induction Module</a></li>
						<li><a href="../modulePages/aiFundamentals.html">Understanding Artificial Intelligence</a></li>
						<li><a href="../modulePages/numericalAnalysis.html">Numerical Analysis</a></li>
						<li><a href="../modulePages/machineLearning.html">Machine Learning</a></li>
						<li><a href="../modulePages/knowledgeRep.html">Knowledge Representation and Reasoning</a></li>
						<li><a href="../modulePages/intelligentAgents.html">Intelligent Agents</a></li>
						<li><a href="../modulePages/researchMethods.html">Research Methods and Professional Practice</a></li>
						<li><a href="../modulePages/masterThesis.html">MSc Computing Project</a></li>
						<li><a href="../projects.html">Projects</a></li>
					</ul>
				</nav>
					<div id="main">
						<div class="inner">
							<h1>Project 4: Maze Solver Model Builder (Reinforcement Learning)</h1>
							<ul class="actions small">
								<li><a href="project4Intro.html" class="button primary">Introduction</a></li>
								<li><a href="project4Part1.html" class="button primary">Part 1</a></li>
								<li><a href="project4Part2.html" class="button primary disabled">Part 2</a></li>
								<li><a href="project4Part3.html" class="button primary">Part 3</a></li>
								<li><a href="project4Result.html" class="button primary">Results</a></li>
							</ul>
							<div class="modern-background">									
								<h2>Reinforcement Learning to resolve the maze</h2>
								<p>The mazeBuilder.py aims at:</p>
								<ul>
									<li>Implementing a Q-learning algorithm to train an agent for solving mazes.</li>
									<li>Incorporating movement rewards and penalties, including incentives for exploration.</li>
									<li>Allowing configuration of training parameters such as learning rate (alpha), discount factor (gamma), and exploration rate (epsilon).</li>
									<li>Tracking of training metrics for performance evaluation and bug fixing.</li>
									<li>Supporting saving the trained Q-table model for future use and testing.</li>
								</ul>

								<p>Below is a detailed overview of the key training parameters I used to build and refine my RL model. Each parameter is carefully tuned to strike a balance between exploration, exploitation, and overall performance. </p>								
								<table style="border-collapse: collapse; max-width: 100%; text-align: left;">
									<thead>
										<tr style="background-color: #8d8d8d; color: #333333;">
											<th style="padding: 10px; border: 1px solid #ddd;">Parameter</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Value</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Description</th>
											<th style="padding: 10px; border: 1px solid #ddd;">How it works</th>
										</tr>
									</thead>
									<tbody>
										<!-- EPISODES -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPISODES</td>
											<td style="padding: 10px; border: 1px solid #ddd;">1'000-2'000 per maze in the test data set</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Number of training episodes to run the agent.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Larger values typically allow more exploration and learning.
											</td>
										</tr>
										<!-- ALPHA -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">ALPHA</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.0125</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Learning rate: controls how quickly the agent updates its Q-values.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Higher values can speed up learning but risk overshooting.
											</td>
										</tr>
										<!-- GAMMA -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">GAMMA</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.99</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Discount factor: determines how the agent values future rewards over immediate ones.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Closer to 1.0 means the agent heavily favors long-term gains.
											</td>
										</tr>
										<!-- EPSILON_START -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPSILON_START</td>
											<td style="padding: 10px; border: 1px solid #ddd;">1.0</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Starting exploration rate for an epsilon-greedy policy.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Initially explores fully (random actions).
											</td>
										</tr>
										<!-- EPSILON_END -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPSILON_END</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.15</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Minimum exploration rate after decay.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Agent still explores occasionally even at low epsilon.
											</td>
										</tr>
										<!-- EPSILON_DECAY -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPSILON_DECAY</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.9998</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Rate at which epsilon decreases each episode.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Slowly reduces exploration over time.
											</td>
										</tr>
										<!-- TEMPERATURE_START -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">TEMPERATURE_START</td>
											<td style="padding: 10px; border: 1px solid #ddd;">1.0</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Initial temperature for softmax action selection (if used).
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Higher temperature means more random exploration initially.
											</td>
										</tr>
										<!-- TEMPERATURE_END -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">TEMPERATURE_END</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.15</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Final temperature after decay.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Encourages more deterministic choices later in training.
											</td>
										</tr>
										<!-- TEMPERATURE_DECAY -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">TEMPERATURE_DECAY</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.9998</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Rate at which the temperature decreases each episode.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Gradually shifts behavior from exploratory to exploitative.
											</td>
										</tr>
										<!-- MAX_STEPS -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">MAX_STEPS</td>
											<td style="padding: 10px; border: 1px solid #ddd;">1'000</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Maximum steps allowed in a single episode.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Prevents episodes from running indefinitely.
											</td>
										</tr>
										<!-- MAX_STUCK_STEPS -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">MAX_STUCK_STEPS</td>
											<td style="padding: 10px; border: 1px solid #ddd;">20</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Maximum consecutive steps with no movement.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Helps the agent avoid getting stuck in loops or dead ends.
											</td>
										</tr>
									</tbody>
								</table>
								

								<p>Below is a detailed overview of the reward structure I implemented during training. Each component reflects how the agent is incentivized or penalized for specific actions and outcomes, guiding it to navigate the maze more effectively and reach the goal. </p>

								<table style="border-collapse: collapse; max-width: 100%; text-align: left;">
									<thead>
										<tr style="background-color: #8d8d8d; color: #333333;">
											<th style="padding: 10px; border: 1px solid #ddd;">Parameter</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Value</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Description</th>
											<th style="padding: 10px; border: 1px solid #ddd;">How it works</th>
										</tr>
									</thead>
									<tbody>
										<!-- Base Step Penalty -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Base Step Penalty</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-0.1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Applies to every step taken, preventing the agent from wandering indefinitely.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Encourages more purposeful movement to reach the goal.
											</td>
										</tr>
										<!-- Goal Reward -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Goal Reward</td>
											<td style="padding: 10px; border: 1px solid #ddd;">+1000</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Granted when the agent reaches the goal location.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Strong incentive to navigate successfully.
											</td>
										</tr>
										<!-- Distance-Based Closer -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Distance-Based: Closer</td>
											<td style="padding: 10px; border: 1px solid #ddd;">+2</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Added when the new position is closer to the goal than the old position.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Rewards steps that move the agent toward the goal.
											</td>
										</tr>
										<!-- Distance-Based Farther -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Distance-Based: Farther</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Subtracted if the agent moves away from the goal.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Penalizes counterproductive moves.
											</td>
										</tr>
										<!-- Hitting Wall or Out-of-Bounds -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Collision (Wall/Out-of-Bounds)</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Occurs if the agent attempts to move into a wall or outside the maze.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Discourages invalid moves.
											</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Visiting New State</td>
											<td style="padding: 10px; border: 1px solid #ddd;">+1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Reward for exploring unvisited cells.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Encourages exploration of unknown territory.
											</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Revisiting State Penalty</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Applied if the agent re-enters a previously visited state.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Discourages oscillation or toggling between the same cells.
											</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Loop Detection Threshold</td>
											<td style="padding: 10px; border: 1px solid #ddd;">20</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Looks for patterns in steps.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Help the agent to avoid looping.
											</td>
										</tr>
									</tbody>
								</table>
								

								<p>The following table shows my attempts at training the agent in maze solving:</p>
								
								<table style="border-collapse: collapse; max-width: 100%; text-align: left;">
									<thead>
										<tr style="background-color: #8d8d8d; color: #333333;">
											<th style="padding: 10px; border: 1px solid #ddd;">Attempt</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Parameters / Log Mention</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Success Rate</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">First run with a higher base penalty, small goal reward, many steps wandering.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.05%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">2</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Adjusted parameters (some increased goal reward, smaller step penalty).</td>
											<td style="padding: 10px; border: 1px solid #ddd;">6.15%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">3</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Another set of reward changes adjusting all rewards slightly.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">8.75%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">4</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Eventually changed to a bigger goal reward to 1000 after and reduced epsilon and temperature decay from 0.999 to 0.9999.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">24.14%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">5</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Bigger reward for goal reaching, increased epsilon and temperature decay (0.9995) while also increasing rewards for exploration.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">39.38%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">6</td>
											<td style="padding: 10px; border: 1px solid #ddd;">I increased the rewards for exploration further and adjusted the max step count as well as the learning parameters.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">55.80%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">7</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Changed the alpha (0.6) and gamma value (0.96).</td>
											<td style="padding: 10px; border: 1px solid #ddd;">57.77%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">8</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Removed penalties for walking into walls and moving further away from the goal while also adjusting all reward sizes to simplify the model. Adjust alpha (0.44) and gamma value (0.9).</td>
											<td style="padding: 10px; border: 1px solid #ddd;">58.59%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">9</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Started to use curriculum learning (increasing complexity over time).</td>
											<td style="padding: 10px; border: 1px solid #ddd;">76.45%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">10</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Further decreased complexity for the starting mazes until they actually where an empty space with start end endpoint.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">87.47%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">11</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Fine tuning of parameteres and a lot of trial and error.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">89.93%</td>
										</tr>
									</tbody>
								</table>
								<p>During my training, I initially ran into a significant problem: my agent was racking up high rewards without truly solving the mazes (see video below). I realized it was exploiting a classic “oscillation” loophole, moving back and forth to collect a net positive reward by gaining +6 for moving closer and only −1 for moving farther away. To fix this, I added a penalty for revisiting any cell, ensuring the agent wouldn’t profit by simply toggling positions.</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/mazeSolver1.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>I continued by further tweaking and changing the rewards and learning parameteres. This part was very hard as it was not always clear why the agent acted in a specific way and how the changes of parameteres affected it. I decided to break the process down by starting with a very low complexity environment and constantly increasing the complexity. My first attempt was to build an agent that can simply walk from start to finish in an empty maze and then increasing the complexity of the maze over time.</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/mazeSolver2.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>
								<p>After some training with mazes and obsticle arenas I tested the agent on new maps with new obsticles and it completed them all successfully (see video below). The agent may not have used the best route but it achieved to reach the goal. Further finetuning, more learning data and training are required. I will continue to train the model with increasing difficulting using cirriculum learning.</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/mazeSolver3.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>
								<p>Some further fine-tuning of rewards as well as testing different model training parameters helped to make the agent capable of solving parkours that were a bit more complex. Note that the agent does still not follow any meaningful path and requires more fine-tuning:</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/mazeSolver4.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p><strong>The Code:</strong></p>

								<pre><code class="language-python">
import os
import json
import pickle
import random
from collections import deque
import numpy as np
import matplotlib.pyplot as plt

# ===================================================
# Configuration
# ===================================================

CONFIG = {
	# Maze reward/penalty parameters
	"step_penalty": -0.1, #was -0.1
	"goal_reward": 1000, #was 1000
	"reward_closer": 2, #was 2
	"reward_farther": -1, #was -1
	"wall_penalty": -1, #was -1
	"exploration_bonus": 1, #was 1
	"revisit_penalty": -1, #was -1
	"loop_detection_threshold": 1000, #was 1000

	# Learning parameters
	"episodes": 37000, #was 12000
	"alpha": 0.125, #was 0.0125
	"gamma": 0.99,
	"epsilon_start": 1.0,
	"epsilon_end": 0.15, #was 0.15
	"epsilon_decay": 0.9998,
	"temperature_start": 1.0,
	"temperature_end": 0.15, #was 0.15
	"temperature_decay": 0.9998,
	"max_steps": 1000,
	"max_stuck_steps": 1000, #was 1000

	# Q-value initialization
	"optimistic_value": 10,

	# Experience replay
	"use_experience_replay": True,
	"replay_buffer_size": 100000,
	"replay_batch_size": 64,
	"replay_frequency": 4,

	# Prioritized sweeping
	"use_prioritized_sweeping": True,
	"priority_threshold": 0.02,

	# Curriculum learning
	"use_curriculum_learning": True,
	"curriculum_phases": 6
}

# ===================================================
# Maze Environment
# ===================================================

class MazeEnv:
	def __init__(self, maze, start, goal):
		self.maze = np.array(maze)
		self.start = tuple(start)
		self.goal = tuple(goal)
		# Define actions: right, down, left, up
		self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]
		self.reset()
		
		# Calculate maze complexity metrics
		self.complexity = self.calculate_complexity()

	def calculate_complexity(self):
		"""Calculate a complexity score for this maze"""
		# Manhattan distance from start to goal
		manhattan_dist = abs(self.start[0] - self.goal[0]) + abs(self.start[1] - self.goal[1])
		
		# Count number of walls
		wall_count = np.sum(self.maze == 1)
		total_cells = self.maze.shape[0] * self.maze.shape[1]
		wall_ratio = wall_count / total_cells
		
		# Combine metrics
		return manhattan_dist * (1 + wall_ratio)

	def reset(self):
		self.position = self.start
		self.visited_states = {self.start: 1}  # Track visit count
		self.path = [self.start]  # Track full path for analysis
		return self.position

	def step(self, action):
		x, y = self.position
		dx, dy = self.actions[action]
		nx, ny = x + dx, y + dy

		# Check boundaries/walls
		if (0 <= nx < self.maze.shape[0] and 0 <= ny < self.maze.shape[1] and self.maze[nx, ny] == 0):
			new_position = (nx, ny)
		else:
			new_position = self.position

		done = False

		# Base step reward
		reward = CONFIG["step_penalty"]

		# Check if goal is reached
		if new_position == self.goal:
			reward = CONFIG["goal_reward"]
			done = True
		else:
			# Distance-based reward/penalty
			old_dist = abs(x - self.goal[0]) + abs(y - self.goal[1])
			new_dist = abs(new_position[0] - self.goal[0]) + abs(new_position[1] - self.goal[1])
			if new_dist < old_dist:
				reward += CONFIG["reward_closer"]
			elif new_dist > old_dist:
				reward += CONFIG["reward_farther"]

			# Penalty for hitting a wall or out-of-bounds
			if new_position == self.position and (nx, ny) != self.position:
				reward += CONFIG["wall_penalty"]

			# Exploration bonus or revisit penalty with increasing penalty for frequently visited states
			if new_position not in self.visited_states:
				reward += CONFIG["exploration_bonus"]
				self.visited_states[new_position] = 1
			else:
				# Increasing penalty for frequently visited states
				visit_count = self.visited_states[new_position]
				revisit_penalty = CONFIG["revisit_penalty"] * (1 + 0.1 * visit_count)
				reward += revisit_penalty
				self.visited_states[new_position] += 1

		self.position = new_position
		self.path.append(new_position)
		return new_position, reward, done

# ===================================================
# Helper Functions
# ===================================================

def load_mazes(folder):
	mazes = []
	for fname in os.listdir(folder):
		if fname.endswith(".json"):
			path = os.path.join(folder, fname)
			with open(path, "r") as f:
				mazes.append(json.load(f))
	return mazes

def sort_mazes_by_complexity(mazes):
	"""Sort mazes by estimated complexity for curriculum learning"""
	sorted_mazes = []
	for maze_data in mazes:
		env = MazeEnv(maze_data["maze"], maze_data["start"], maze_data["goal"])
		sorted_mazes.append((env.complexity, maze_data))
	
	# Sort by complexity score
	sorted_mazes.sort(key=lambda x: x[0])
	return sorted_mazes

def select_action(q_values, epsilon, temperature):
	if np.random.rand() < epsilon:
		return np.random.randint(len(q_values))
	
	# Softmax action selection with temperature
	max_q = max(q_values)
	exp_values = [np.exp((q - max_q) / max(temperature, 1e-8)) for q in q_values]
	sum_exp = sum(exp_values)
	probs = [val / sum_exp for val in exp_values]
	return np.random.choice(len(q_values), p=probs)

def get_q_values(q_table, state, num_actions=4):
	"""Return Q-values for a given state with optimistic initialization."""
	if state not in q_table:
		q_table[state] = [CONFIG["optimistic_value"]] * num_actions
	return q_table[state]

def experience_replay(q_table, experience_buffer, batch_size, alpha, gamma):
	"""Learn from randomly sampled past experiences"""
	if len(experience_buffer) < batch_size:
		return q_table
	
	# Sample random batch of experiences
	batch = random.sample(experience_buffer, batch_size)
	
	for state, action, reward, next_state, done in batch:
		q_values = get_q_values(q_table, state)
		if done:
			# Terminal state update
			new_q = q_values[action] + alpha * (reward - q_values[action])
		else:
			# Standard Q-learning update
			next_q_values = get_q_values(q_table, next_state)
			best_next_q = max(next_q_values)
			new_q = q_values[action] + alpha * (reward + gamma * best_next_q - q_values[action])
		
		q_values[action] = new_q
		q_table[state] = q_values
	
	return q_table

def prioritized_sweeping(q_table, state_transitions, alpha, gamma, priority_threshold=0.5):
	"""Update Q-values for states with significant changes"""
	if not state_transitions:
		return q_table
	
	# Sort transitions by absolute TD error (priority)
	for state, action, reward, next_state, done in state_transitions:
		q_values = get_q_values(q_table, state)
		
		if done:
			td_error = abs(reward - q_values[action])
		else:
			next_q_values = get_q_values(q_table, next_state)
			best_next_q = max(next_q_values)
			td_error = abs(reward + gamma * best_next_q - q_values[action])
		
		# Only update if error is significant
		if td_error > priority_threshold:
			if done:
				new_q = q_values[action] + alpha * (reward - q_values[action])
			else:
				new_q = q_values[action] + alpha * (reward + gamma * best_next_q - q_values[action])
			
			q_values[action] = new_q
			q_table[state] = q_values
	
	return q_table

def detect_early_stopping(rewards, episode_window=500, improvement_threshold=0.001):
	"""Disable early stopping to allow full training"""
	return False  # Never trigger early stopping

def visualize_sample_mazes(sorted_mazes, num_to_visualize=3):
	"""Visualize a few sample mazes to verify they're solvable"""
	os.makedirs("maze_visualizations", exist_ok=True)
	
	for i in range(min(num_to_visualize, len(sorted_mazes))):
		complexity, maze_data = sorted_mazes[i]
		maze = np.array(maze_data["maze"])
		start = tuple(maze_data["start"])
		goal = tuple(maze_data["goal"])
		
		plt.figure(figsize=(8, 8))
		plt.imshow(maze, cmap='binary')
		plt.scatter(start[1], start[0], c='green', s=200, marker='o', label='Start')
		plt.scatter(goal[1], goal[0], c='red', s=200, marker='*', label='Goal')
		plt.title(f'Maze #{i+1} - Complexity: {complexity:.2f}')
		plt.legend()
		plt.grid(False)
		plt.savefig(f"maze_visualizations/maze_sample_{i+1}.png")
		plt.close()
	
	print(f"Visualized {num_to_visualize} sample mazes in 'maze_visualizations' folder")

def train_q_learning(
	mazes,
	episodes=CONFIG["episodes"],
	alpha=CONFIG["alpha"],
	gamma=CONFIG["gamma"],
	epsilon_start=CONFIG["epsilon_start"],
	epsilon_end=CONFIG["epsilon_end"],
	epsilon_decay=CONFIG["epsilon_decay"],
	temperature_start=CONFIG["temperature_start"],
	temperature_end=CONFIG["temperature_end"],
	temperature_decay=CONFIG["temperature_decay"],
	max_steps=CONFIG["max_steps"],
	max_stuck_steps=CONFIG["max_stuck_steps"]
):
	# For tracking training progress
	q_table = {}
	total_rewards = []
	success_count = 0
	steps_per_episode = []
	epsilon_values = []
	success_rates = []
	mean_reward_per_1000 = []

	epsilon = epsilon_start
	temperature = temperature_start
	
	# Sort mazes for curriculum learning if enabled
	if CONFIG["use_curriculum_learning"]:
		sorted_mazes = sort_mazes_by_complexity(mazes)
		print(f"Sorted {len(sorted_mazes)} mazes by complexity for curriculum learning")
		
		# Visualize a few sample mazes to verify they're solvable
		visualize_sample_mazes(sorted_mazes, num_to_visualize=3)
		
		# Print complexity information for first few mazes
		print("Maze complexity distribution:")
		for i, (complexity, maze) in enumerate(sorted_mazes):
			print(f"  Maze {i+1}: Complexity score {complexity:.2f}")

		sorted_mazes = [m[1] for m in sorted_mazes]  # Extract just the maze data
	else:
		sorted_mazes = mazes
	
	# Experience replay buffer
	replay_buffer = deque(maxlen=CONFIG["replay_buffer_size"])
	
	# Create a directory for saving figures
	os.makedirs("training_plots", exist_ok=True)

	for ep in range(1, episodes + 1):
		# Curriculum learning: gradually introduce more complex mazes
		if CONFIG["use_curriculum_learning"]:
			# Determine which subset of mazes to use based on training progress
			phase = min(int((ep / episodes) * CONFIG["curriculum_phases"]), CONFIG["curriculum_phases"] - 1)
			maze_subset_size = max(1, int((phase + 1) * len(sorted_mazes) / CONFIG["curriculum_phases"]))
			maze_subset = sorted_mazes[:maze_subset_size]
			maze_data = random.choice(maze_subset)
		else:
			maze_data = random.choice(mazes)
		
		env = MazeEnv(maze_data["maze"], maze_data["start"], maze_data["goal"])
		state = env.reset()
		episode_reward = 0
		done = False
		steps = 0
		
		# For loop detection
		state_history = deque(maxlen=20)  # Larger history for better loop detection
		stuck_count = 0
		
		# For prioritized sweeping
		state_transitions = []

		while not done and steps < max_steps:
			steps += 1
			state_history.append(state)

			# Detect if stuck in a loop
			if state_history.count(state) > CONFIG["loop_detection_threshold"]:
				action = np.random.randint(len(env.actions))
			else:
				q_values = get_q_values(q_table, state)
				action = select_action(q_values, epsilon, temperature)

			next_state, reward, done = env.step(action)
			episode_reward += reward
			
			# Add to experience replay buffer
			replay_buffer.append((state, action, reward, next_state, done))
			
			# Track state transitions for prioritized sweeping
			state_transitions.append((state, action, reward, next_state, done))

			# Update stuck counter
			if next_state == state:
				stuck_count += 1
			else:
				stuck_count = 0

			# Force a random move if stuck
			if stuck_count >= max_stuck_steps and not done:
				action = np.random.randint(len(env.actions))
				next_state, forced_reward, forced_done = env.step(action)
				episode_reward += forced_reward
				done = forced_done
				stuck_count = 0
				
				# Add forced move to replay buffer
				replay_buffer.append((state, action, forced_reward, next_state, forced_done))
				state_transitions.append((state, action, forced_reward, next_state, forced_done))

			# Standard Q-learning update
			q_values = get_q_values(q_table, state)
			old_q = q_values[action]
			next_q_values = get_q_values(q_table, next_state)
			best_next_q = max(next_q_values)
			new_q = old_q + alpha * (reward + gamma * best_next_q - old_q)
			q_values[action] = new_q
			q_table[state] = q_values

			state = next_state
		
		# Experience replay after episode if enabled
		if CONFIG["use_experience_replay"] and ep % CONFIG["replay_frequency"] == 0:
			q_table = experience_replay(
				q_table, 
				replay_buffer, 
				CONFIG["replay_batch_size"], 
				alpha, 
				gamma
			)
		
		# Prioritized sweeping if enabled
		if CONFIG["use_prioritized_sweeping"]:
			q_table = prioritized_sweeping(
				q_table, 
				state_transitions, 
				alpha, 
				gamma, 
				CONFIG["priority_threshold"]
			)
		
		# Track episode results
		total_rewards.append(episode_reward)
		steps_per_episode.append(steps)
		epsilon_values.append(epsilon)
		
		if done and episode_reward > 0:  # Successfully reached the goal
			success_count += 1
		
		# Calculate current success rate
		current_success_rate = 100.0 * success_count / ep
		success_rates.append(current_success_rate)

		# Decay exploration parameters
		epsilon = max(epsilon_end, epsilon * epsilon_decay)
		temperature = max(temperature_end, temperature * temperature_decay)

		# Reporting
		if ep % 1000 == 0:
			# Calculate mean reward over last 1000 episodes
			last_1000_mean = np.mean(total_rewards[-1000:])
			mean_reward_per_1000.append(last_1000_mean)
			
			print(f"Episode {ep}/{episodes}: Reward={episode_reward:.1f}, Steps={steps}, "
					f"Epsilon={epsilon:.3f}, Success Rate={current_success_rate:.2f}%")
			
			if CONFIG["use_curriculum_learning"]:
				print(f"  Current Maze Complexity: {env.complexity:.2f}, "
						f"Current Phase: {phase+1}/{CONFIG['curriculum_phases']}")
			
			print(f"  Average Last 1000 Rewards: {last_1000_mean:.2f}")
			
			# Plot training progress every 10,000 episodes
			if ep % 10000 == 0:
				plot_training_progress(
					total_rewards,
					steps_per_episode,
					epsilon_values,
					success_rates,
					mean_reward_per_1000,
					ep
				)
	
	# Final statistics
	avg_reward = np.mean(total_rewards)
	success_rate = 100.0 * success_count / episodes
	avg_steps_success = np.mean([s for i, s in enumerate(steps_per_episode) if i < len(total_rewards) and total_rewards[i] > 0]) if success_count > 0 else 0
	
	print("\nTraining Finished!")
	print(f"  Average Reward: {avg_reward:.2f}")
	print(f"  Success Rate: {success_rate:.2f}%")
	print(f"  Average Steps (successful episodes): {avg_steps_success:.2f}")
	
	# Final learning curves
	plot_training_progress(
		total_rewards,
		steps_per_episode,
		epsilon_values,
		success_rates,
		mean_reward_per_1000,
		episodes,
		final=True
	)
	
	return q_table

def plot_training_progress(rewards, steps, epsilons, success_rates, mean_rewards_per_1000, episode, final=False):
	"""Plot training metrics to visualize learning progress"""
	plt.figure(figsize=(15, 12))
	
	# Plot moving average of rewards
	window_size = min(100, len(rewards))
	if window_size > 0:
		smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
		
		plt.subplot(3, 2, 1)
		plt.plot(smoothed_rewards)
		plt.title('Average Reward (Smoothed)')
		plt.xlabel('Episode')
		plt.ylabel('Reward')
		
		# Steps per episode
		plt.subplot(3, 2, 2)
		smoothed_steps = np.convolve(steps, np.ones(window_size)/window_size, mode='valid')
		plt.plot(smoothed_steps)
		plt.title('Steps per Episode (Smoothed)')
		plt.xlabel('Episode')
		plt.ylabel('Steps')
		
		# Epsilon decay
		plt.subplot(3, 2, 3)
		plt.plot(epsilons)
		plt.title('Exploration Rate (Epsilon)')
		plt.xlabel('Episode')
		plt.ylabel('Epsilon')
		
		# Success rate progression
		plt.subplot(3, 2, 4)
		plt.plot(success_rates)
		plt.title('Success Rate Progression')
		plt.xlabel('Episode')
		plt.ylabel('Success Rate (%)')
		plt.ylim(0, 100)
		
		# Mean reward per 1000 episodes
		plt.subplot(3, 2, 5)
		plt.plot(range(1000, len(rewards) + 1, 1000), mean_rewards_per_1000)
		plt.title('Mean Reward per 1000 Episodes')
		plt.xlabel('Episode')
		plt.ylabel('Mean Reward')
		
		# Reward histogram 
		plt.subplot(3, 2, 6)
		plt.hist(rewards, bins=50)
		plt.title('Reward Distribution')
		plt.xlabel('Reward')
		plt.ylabel('Frequency')
	
	plt.tight_layout()
	
	# Save figure
	if final:
		plt.savefig(f"training_plots/final_training_progress.png")
	else:
		plt.savefig(f"training_plots/training_progress_ep{episode}.png")
	
	plt.close()

def save_q_table(q_table, output_folder, filename):
	"""Save the Q-table to disk."""
	os.makedirs(output_folder, exist_ok=True)
	filepath = os.path.join(output_folder, filename)
	with open(filepath, "wb") as f:
		pickle.dump(q_table, f)
	print(f"Q-table saved to {filepath}")

def visualize_maze_policy(q_table, maze_data, output_folder="maze_visualizations"):
	"""Visualize the learned policy for a specific maze"""
	os.makedirs(output_folder, exist_ok=True)
	
	maze = np.array(maze_data["maze"])
	start = tuple(maze_data["start"])
	goal = tuple(maze_data["goal"])
	
	# Create a visualization of the maze with policy arrows
	plt.figure(figsize=(12, 12))
	plt.imshow(maze, cmap='binary')
	
	# Action directions for visualization
	arrows = ['→', '↓', '←', '↑']
	arrow_colors = ['blue', 'green', 'red', 'purple']
	
	# Plot arrows for each navigable cell
	for i in range(maze.shape[0]):
		for j in range(maze.shape[1]):
			if maze[i, j] == 0:  # If navigable
				state = (i, j)
				if state in q_table:
					q_values = q_table[state]
					best_action = np.argmax(q_values)
					
					# Add Q-value annotation
					plt.text(j, i-0.3, f"{max(q_values):.1f}", 
								ha='center', va='center', 
								color='black', fontsize=7)
					
					# Plot arrow for best action
					plt.text(j, i, arrows[best_action], 
								ha='center', va='center', 
								color=arrow_colors[best_action],
								fontsize=12, fontweight='bold')
	
	# Mark start and goal
	plt.scatter(start[1], start[0], c='green', s=200, marker='o', label='Start')
	plt.scatter(goal[1], goal[0], c='red', s=200, marker='*', label='Goal')
	
	plt.title('Maze Policy Visualization')
	plt.legend()
	plt.grid(False)
	
	# Save the visualization
	plt.savefig(f"{output_folder}/policy_visualization.png")
	plt.close()

def visualize_training_metrics(total_rewards, success_rates, output_folder="training_analysis"):
	"""Create detailed visualizations of training metrics"""
	os.makedirs(output_folder, exist_ok=True)
	
	# Analyze reward distribution
	plt.figure(figsize=(15, 8))
	
	# Plot reward histogram 
	plt.subplot(1, 2, 1)
	plt.hist(total_rewards, bins=50, color='skyblue', edgecolor='black')
	plt.axvline(np.mean(total_rewards), color='r', linestyle='--', linewidth=1, 
				label=f'Mean: {np.mean(total_rewards):.2f}')
	plt.axvline(np.median(total_rewards), color='g', linestyle='--', linewidth=1,
				label=f'Median: {np.median(total_rewards):.2f}')
	plt.title('Total Reward Distribution')
	plt.xlabel('Reward')
	plt.ylabel('Frequency')
	plt.legend()
	
	# Success rate over time
	plt.subplot(1, 2, 2)
	plt.plot(success_rates, color='green')
	plt.title('Success Rate Progression')
	plt.xlabel('Episode')
	plt.ylabel('Success Rate (%)')
	plt.ylim(0, 100)
	
	plt.tight_layout()
	plt.savefig(f"{output_folder}/training_metrics_analysis.png")
	plt.close()

# ===================================================
# Main
# ===================================================

if __name__ == "__main__":
	MAZE_FOLDER = "mazes"
	MODEL_FOLDER = "model"
	MODEL_FILE = "q_table.pkl"

	print("Loading mazes for training...")
	mazes = load_mazes(MAZE_FOLDER)
	print(f"Loaded {len(mazes)} maze(s).")

	print("Starting Q-learning training with enhanced features...")
	print(f"  - Experience Replay: {'Enabled' if CONFIG['use_experience_replay'] else 'Disabled'}")
	print(f"  - Prioritized Sweeping: {'Enabled' if CONFIG['use_prioritized_sweeping'] else 'Disabled'}")
	print(f"  - Curriculum Learning: {'Enabled' if CONFIG['use_curriculum_learning'] else 'Disabled'}")
	
	q_table = train_q_learning(mazes=mazes)

	save_q_table(q_table, MODEL_FOLDER, MODEL_FILE)
	
	# Visualize the policy for a sample maze
	if mazes:
		print("Generating policy visualization for a sample maze...")
		visualize_maze_policy(q_table, mazes[0])
		
		# Create additional analysis visualizations
		print("Generating additional training analysis visualizations...")
		visualize_maze_policy(q_table, mazes[-1], output_folder="maze_visualizations")
	
	print("Training complete! You're ready to go solve some mazes!")
									</code></pre>
							</div>
						</div>
					</div>
				</div>

			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>
			<script src="../assets/js/darkmode.js"></script>

	</body>
</html>