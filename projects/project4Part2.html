<!DOCTYPE HTML>
<html>
	<head>
		<title>e-Portfolio Elias - Projects Page</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
			<div id="wrapper">
				<header id="header">
					<div class="inner">
						<div class="button-container">
							<a href="../index.html" class="logo home-button">
								<span class="symbol"><img src="../images/home-icon.png" alt="" /></span><span class="title">Home</span>
							</a>
							<a href="../projects.html" class="logo back-button">
								<span class="symbol"><img src="../images/back-icon.png" alt="" /></span><span class="title">Back</span>
							</a>
						</div>
							<nav>
								<ul>
									<li><a href="#menu">Menu</a></li>
								</ul>
							</nav>
					</div>
				</header>
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<button id="darkModeToggle">Toggle Dark Mode</button>
						<br>
						<br>
						<li><a href="../index.html">Home</a></li>
						<li><a href="../modulePages/inductionModule.html">Induction Module</a></li>
						<li><a href="../modulePages/aiFundamentals.html">Understanding Artificial Intelligence</a></li>
						<li><a href="../modulePages/numericalAnalysis.html">Numerical Analysis</a></li>
						<li><a href="../modulePages/machineLearning.html">Machine Learning</a></li>
						<li><a href="../modulePages/knowledgeRep.html">Knowledge Representation and Reasoning</a></li>
						<li><a href="../modulePages/intelligentAgents.html">Intelligent Agents</a></li>
						<li><a href="../modulePages/researchMethods.html">Research Methods and Professional Practice</a></li>
						<li><a href="../modulePages/masterThesis.html">MSc Computing Project</a></li>
						<li><a href="../projects.html">Projects</a></li>
					</ul>
				</nav>
					<div id="main">
						<div class="inner">
							<h1>Project 4: Maze Solver Model Builder (Reinforcement Learning)</h1>
							<ul class="actions small">
								<li><a href="project4Intro.html" class="button primary">Introduction</a></li>
								<li><a href="project4Part1.html" class="button primary">Part 1</a></li>
								<li><a href="project4Part2.html" class="button primary disabled">Part 2</a></li>
								<li><a href="project4Part3.html" class="button primary">Part 3</a></li>
								<li><a href="project4Result.html" class="button primary">Results</a></li>
							</ul>
							<div class="modern-background">									
								<h2>Reinforcement Learning to resolve the maze</h2>
								<p>The mazeBuilder.py aims at:</p>
								<ul>
									<li>Implementing a Q-learning algorithm to train an agent for solving mazes.</li>
									<li>Incorporating movement rewards and penalties, including incentives for exploration.</li>
									<li>Allowing configuration of training parameters such as learning rate (alpha), discount factor (gamma), and exploration rate (epsilon).</li>
									<li>Tracking of training metrics for performance evaluation and bug fixing.</li>
									<li>Supporting saving the trained Q-table model for future use and testing.</li>
								</ul>

								<p>Below is a detailed overview of the key training parameters I used to build and refine my RL model. Each parameter is carefully tuned to strike a balance between exploration, exploitation, and overall performance. </p>								
								<table style="border-collapse: collapse; max-width: 100%; text-align: left;">
									<thead>
										<tr style="background-color: #8d8d8d; color: #333333;">
											<th style="padding: 10px; border: 1px solid #ddd;">Dataset (Parameter)</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Predicted Value (FCNN Result)</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Expected Value (Control Value)</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Difference (FCNN Deviation)</th>
										</tr>
									</thead>
									<tbody>
										<!-- EPISODES -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPISODES</td>
											<td style="padding: 10px; border: 1px solid #ddd;">30,000</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Number of training episodes to run the agent.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Larger values typically allow more exploration and learning.
											</td>
										</tr>
										<!-- ALPHA -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">ALPHA</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.45</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Learning rate: controls how quickly the agent updates its Q-values.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Higher values can speed up learning but risk overshooting.
											</td>
										</tr>
										<!-- GAMMA -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">GAMMA</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.99</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Discount factor: determines how the agent values future rewards over immediate ones.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Closer to 1.0 means the agent heavily favors long-term gains.
											</td>
										</tr>
										<!-- EPSILON_START -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPSILON_START</td>
											<td style="padding: 10px; border: 1px solid #ddd;">1.0</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Starting exploration rate for an epsilon-greedy policy.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Initially explores fully (random actions).
											</td>
										</tr>
										<!-- EPSILON_END -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPSILON_END</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.01</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Minimum exploration rate after decay.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Agent still explores occasionally even at low epsilon.
											</td>
										</tr>
										<!-- EPSILON_DECAY -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">EPSILON_DECAY</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.9995</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Rate at which epsilon decreases each episode.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Slowly reduces exploration over time.
											</td>
										</tr>
										<!-- TEMPERATURE_START -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">TEMPERATURE_START</td>
											<td style="padding: 10px; border: 1px solid #ddd;">1.0</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Initial temperature for softmax action selection (if used).
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Higher temperature means more random exploration initially.
											</td>
										</tr>
										<!-- TEMPERATURE_END -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">TEMPERATURE_END</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.01</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Final temperature after decay.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Encourages more deterministic choices later in training.
											</td>
										</tr>
										<!-- TEMPERATURE_DECAY -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">TEMPERATURE_DECAY</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.999</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Rate at which the temperature decreases each episode.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Gradually shifts behavior from exploratory to exploitative.
											</td>
										</tr>
										<!-- MAX_STEPS -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">MAX_STEPS</td>
											<td style="padding: 10px; border: 1px solid #ddd;">4,000</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Maximum steps allowed in a single episode.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Prevents episodes from running indefinitely.
											</td>
										</tr>
										<!-- MAX_STUCK_STEPS -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">MAX_STUCK_STEPS</td>
											<td style="padding: 10px; border: 1px solid #ddd;">10</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Maximum consecutive steps with no movement.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Helps the agent avoid getting stuck in loops or dead ends.
											</td>
										</tr>
									</tbody>
								</table>
								

								<p>Below is a detailed overview of the reward structure I implemented during training. Each component reflects how the agent is incentivized or penalized for specific actions and outcomes, guiding it to navigate the maze more effectively and reach the goal. </p>

								<table style="border-collapse: collapse; max-width: 100%; text-align: left;">
									<thead>
										<tr style="background-color: #8d8d8d; color: #333333;">
											<th style="padding: 10px; border: 1px solid #ddd;">Dataset (Reward Component)</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Predicted Value (FCNN Result)</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Expected Value (Control Value)</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Difference (FCNN Deviation)</th>
										</tr>
									</thead>
									<tbody>
										<!-- Base Step Penalty -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Base Step Penalty</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-0.1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Applies to every step taken, preventing the agent from wandering indefinitely.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Encourages more purposeful movement to reach the goal.
											</td>
										</tr>
										<!-- Goal Reward -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Goal Reward</td>
											<td style="padding: 10px; border: 1px solid #ddd;">+250</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Granted when the agent reaches the goal location.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Strong incentive to navigate successfully.
											</td>
										</tr>
										<!-- Distance-Based Closer -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Distance-Based: Closer</td>
											<td style="padding: 10px; border: 1px solid #ddd;">+5</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Added when the new position is closer to the goal than the old position.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Rewards steps that move the agent toward the goal.
											</td>
										</tr>
										<!-- Distance-Based Farther -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Distance-Based: Farther</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Subtracted if the agent moves away from the goal.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Penalizes counterproductive moves.
											</td>
										</tr>
										<!-- Hitting Wall or Out-of-Bounds -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Collision (Wall/Out-of-Bounds)</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-4</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Occurs if the agent attempts to move into a wall or outside the maze.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Discourages invalid moves.
											</td>
										</tr>
										<!-- Visiting New State -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Visiting New State</td>
											<td style="padding: 10px; border: 1px solid #ddd;">+5</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Reward for exploring unvisited cells.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Encourages exploration of unknown territory.
											</td>
										</tr>
										<!-- Revisiting State Penalty -->
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">Revisiting State Penalty</td>
											<td style="padding: 10px; border: 1px solid #ddd;">-3</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Applied if the agent re-enters a previously visited state.
											</td>
											<td style="padding: 10px; border: 1px solid #ddd;">
												Discourages oscillation or toggling between the same cells.
											</td>
										</tr>
									</tbody>
								</table>
								

								<p></p>
								<p>During my training, I initially ran into a significant problem: my agent was racking up high rewards without truly solving the mazes. I realized it was exploiting a classic “oscillation” loophole, moving back and forth to collect a net positive reward by gaining +6 for moving closer and only −1 for moving farther away. To fix this, I added a penalty for revisiting any cell, ensuring the agent wouldn’t profit by simply toggling positions.</p>
					
								<table style="border-collapse: collapse; max-width: 100%; text-align: left;">
									<thead>
										<tr style="background-color: #8d8d8d; color: #333333;">
											<th style="padding: 10px; border: 1px solid #ddd;">Attempt</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Parameters / Log Mention</th>
											<th style="padding: 10px; border: 1px solid #ddd;">Success Rate</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">1</td>
											<td style="padding: 10px; border: 1px solid #ddd;">First run with a higher base penalty, small goal reward, many steps wandering.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">0.05%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">2</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Adjusted parameters (some increased goal reward, smaller step penalty).</td>
											<td style="padding: 10px; border: 1px solid #ddd;">6.15%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">3</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Another set of reward changes adjusting all rewards slightly.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">8.75%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">4</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Eventually changed to a bigger goal reward to 1000 after and reduced epsilon and temperature decay from 0.999 to 0.9999.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">24.14%</td>
										</tr>
										<tr>
											<td style="padding: 10px; border: 1px solid #ddd;">5</td>
											<td style="padding: 10px; border: 1px solid #ddd;">Bigger reward for goal reaching, increased epsilon and temperature decay (0.9995) while also increasing rewards for exploration.</td>
											<td style="padding: 10px; border: 1px solid #ddd;">39.38%</td>
										</tr>
									</tbody>
								</table>
								<p>The agent still gets stuck after improving the success rate to 39.38% and implementing various measures to avoid this behaviour:</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/mazeSolver1.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>
								

								<pre><code class="language-python">
import os
import json
import numpy as np
import pickle
from collections import deque

class MazeEnv:
	def __init__(self, maze, start, goal):
		self.maze = np.array(maze)
		self.start = tuple(start)
		self.goal = tuple(goal)
		self.position = self.start
		self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # right, down, left, up
		self.previous_direction = None  # To track the previous action
		self.visited_states = set()  # To track visited states

	def reset(self):
		self.position = self.start
		self.previous_direction = None  # Reset the direction tracker
		self.visited_states = set()  # Clear visited states
		return self.position

	def step(self, action):
		x, y = self.position
		dx, dy = self.actions[action]
		nx, ny = x + dx, y + dy

		# Check bounds and walls
		if 0 <= nx < self.maze.shape[0] and 0 <= ny < self.maze.shape[1] and self.maze[nx, ny] == 0:
			self.position = (nx, ny)

		# Check if goal is reached
		done = self.position == self.goal
		reward = 10 if done else -1

		# Movement incentives
		if not done:
			current_distance = abs(x - self.goal[0]) + abs(y - self.goal[1])
			new_distance = abs(nx - self.goal[0]) + abs(ny - self.goal[1])
			if new_distance < current_distance:
				reward += 5  # Reward for getting closer
			elif new_distance > current_distance:
				reward -= 1  # Penalty for moving away
			if self.position == (x, y):  # No movement
				reward -= 10

		# Check for direction change
		if self.previous_direction is not None and self.previous_direction != action:
			reward -= 2  # Penalty for changing direction

		# Reward for visiting a new state
		if self.position not in self.visited_states:
			reward += 5  # Reward for exploring
			self.visited_states.add(self.position)

		# Update the previous direction
		self.previous_direction = action

		return self.position, reward, done

def load_mazes(folder):
	"""Load JSON mazes from the specified folder."""
	files = [f for f in os.listdir(folder) if f.endswith(".json")]
	mazes = []
	for file in files:
		with open(os.path.join(folder, file), "r") as f:
			maze_data = json.load(f)
			mazes.append(maze_data)
	return mazes

def train_q_learning(mazes, episodes=5000, alpha=0.1, gamma=0.9, epsilon=1.0, max_failed_attempts=10, max_steps=5000):
	q_table = {}
	total_rewards = []
	episode_lengths = []
	successes = 0

	for maze_idx, maze_data in enumerate(mazes, start=1):
		env = MazeEnv(maze_data["maze"], maze_data["start"], maze_data["goal"])
		failed_attempts = 0

		print(f"Training on Maze {maze_idx}...")

		for episode in range(episodes):
			state = env.reset()
			done = False
			total_reward = 0
			steps = 0

			while not done:
				steps += 1
				if steps > max_steps:
					print(f"Episode terminated early after {max_steps} steps (likely stuck).")
					failed_attempts += 1
					break

				if np.random.random() < epsilon:
					action = np.random.choice(len(env.actions))  # Explore
				else:
					action = np.argmax(q_table.get(state, [0, 0, 0, 0]))  # Exploit

				next_state, reward, done = env.step(action)
				total_reward += reward

				if state == next_state:
					reward -= 20  # Penalize staying in the same place

				q_values = q_table.get(state, [0, 0, 0, 0])
				q_values[action] += alpha * (
					reward + gamma * max(q_table.get(next_state, [0, 0, 0, 0])) - q_values[action]
				)
				q_table[state] = q_values
				state = next_state

			total_rewards.append(total_reward)
			episode_lengths.append(steps)
			if total_reward >= 10:
				successes += 1

			if failed_attempts >= max_failed_attempts:
				print(f"Skipping Maze {maze_idx} after {failed_attempts} failed attempts.")
				break

			epsilon = max(0.01, epsilon * 0.995)  # Change epsilon faster or slower convergence to exploitation

	avg_reward = np.mean(total_rewards) if total_rewards else 0
	avg_length = np.mean(episode_lengths) if episode_lengths else 0
	success_rate = (successes / (len(mazes) * episodes)) * 100 if len(mazes) > 0 else 0

	print("\nTraining Metrics:")
	print(f"  Average Reward per Episode: {avg_reward:.2f}")
	print(f"  Average Episode Length: {avg_length:.2f} steps")
	print(f"  Success Rate: {success_rate:.2f}%")

	return q_table

def save_model(model, folder, filename):
	"""Save the trained Q-table to a file."""
	os.makedirs(folder, exist_ok=True)
	filepath = os.path.join(folder, filename)
	with open(filepath, "wb") as f:
		pickle.dump(model, f)
	print(f"Model saved to {filepath}")

if __name__ == "__main__":
	# Define paths
	maze_folder = "mazes"
	model_folder = "model"
	model_filename = "q_table.pkl"

	# Load mazes
	print("Loading mazes...")
	mazes = load_mazes(maze_folder)
	print(f"Loaded {len(mazes)} mazes.")

	#training parameters
	episodes = 7000
	alpha = 0.5 # Learning rate higher means more weight on new information and lower means slower learning but smoother convergence
	gamma = 0.99  # Discount factor Long-term planning for high and shortterm for low
	epsilon = 0.9  # Initial exploration rate
	max_steps = 5000 #max steps to avoid infinite loops
	max_failed_attempts = 10

	# Train Q-learning agent
	print("Training Q-learning agent...")
	q_table = train_q_learning(mazes, episodes=episodes, alpha=alpha, gamma=gamma, epsilon=epsilon, max_failed_attempts=max_failed_attempts, max_steps=max_steps)

	# Save the trained model
	print("Saving the trained model...")
	save_model(q_table, model_folder, model_filename)
									</code></pre>
							</div>
						</div>
					</div>
				</div>

			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>
			<script src="../assets/js/darkmode.js"></script>

	</body>
</html>