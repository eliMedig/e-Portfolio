<!DOCTYPE HTML>
<html>
	<head>
		<title>e-Portfolio Elias - Projects Page</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
			<div id="wrapper">
				<header id="header">
					<div class="inner">
						<div class="button-container">
							<a href="../index.html" class="logo home-button">
								<span class="symbol"><img src="../images/home-icon.png" alt="" /></span><span class="title">Home</span>
							</a>
							<a href="../projects.html" class="logo back-button">
								<span class="symbol"><img src="../images/back-icon.png" alt="" /></span><span class="title">Back</span>
							</a>
						</div>
							<nav>
								<ul>
									<li><a href="#menu">Menu</a></li>
								</ul>
							</nav>
					</div>
				</header>
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<button id="darkModeToggle">Toggle Dark Mode</button>
						<br>
						<br>
						<li><a href="../index.html">Home</a></li>
						<li><a href="../modulePages/inductionModule.html">Induction Module</a></li>
						<li><a href="../modulePages/aiFundamentals.html">Understanding Artificial Intelligence</a></li>
						<li><a href="../modulePages/numericalAnalysis.html">Numerical Analysis</a></li>
						<li><a href="../modulePages/machineLearning.html">Machine Learning</a></li>
						<li><a href="../modulePages/knowledgeRep.html">Knowledge Representation and Reasoning</a></li>
						<li><a href="../modulePages/intelligentAgents.html">Intelligent Agents</a></li>
						<li><a href="../modulePages/researchMethods.html">Research Methods and Professional Practice</a></li>
						<li><a href="../modulePages/masterThesis.html">MSc Computing Project</a></li>
						<li><a href="../projects.html">Projects</a></li>
					</ul>
				</nav>
					<div id="main">
						<div class="inner">
							<h1>Project 6: Reinforcement Gaming Agent</h1>
							<ul class="actions small">
								<li><a href="project6Intro.html" class="button primary">Introduction</a></li>
								<li><a href="project6Part1.html" class="button primary">Part 1</a></li>
								<li><a href="project6Part2.html" class="button primary">Part 2</a></li>
								<li><a href="project6Part3.html" class="button primary">Part 3</a></li>
								<li><a href="project6Result.html" class="button primary disabled">Results</a></li>
							</ul>
							<div class="modern-background">
								<h2>Results</h2>
								<p>I began training the Reinforcement Learning (RL) agent using well-established best-practice parameters as a foundation. From there, I continuously refined the model by adjusting, experimenting, and fine-tuning key hyperparameters to enhance performance. This results section showcases key insights from the training process, along with videos demonstrating the agent's learning progress and evolving strategies.</p>
								
								<p>The initial agent exhibited fascinating behavior right from the start. In my first attempt at setting up the environment, I overlooked properly defining the game boundaries, which led to the following unexpected outcome:</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult1.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>After correcting the underlying issue and retraining the agent, I observed another interesting pattern. Rather than navigating dynamically, the agent still moveed towards the boundaries and remained there and this was after a new training session:</p>

								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult2.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>I continued training the agent, introducing various changes to the code along the way. I adjusted the reward structure, introduced new incentives (for example a penalty for staying at the wall), fine-tuned hyperparameters, and extended the training duration by increasing the number of episodes.</p>
								<p>The following video showcases a trained agent playing the game, with an overlaid heatmap representing its focus areas (trained over 3000 episodes). This visualization provides insight into how the agent perceives its environment. While the agent clearly reacts to enemies, its behavior is still not optimal. Instead of avoiding them strategically, it appears to move directly into their trajectory.</p>

								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult3.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>I decided to continue training and evaluating the agent and recorded some of the training sessions:</p>

								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult4.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>After a lot of fine-tuning and trial and error the agent showcased a better understanding of its environment which can be seen by how he avoiding enemies more efficiently but the agent still struggled with situations where the enemies were to fast and where it therefore did not have enough reaction time. It is important to mention that this was achieved with only 100 episodes. Further training may increase the accuracy of the agent:</p>

								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult5.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>After training the agent for 100 episodes, I adjusted the reward structure to put more emphasis on avoiding enemies. I then continued training the same agent for an additional 500 episodes with these modified rewards. Itâ€™s important to note that changing the reward structure during training carries some risks. The agent relies on these rewards to navigate the environment, such modifications might introduce unexpected challenges.</p>

								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult6.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>I decided to further investigate how I could reward the agent for avoiding enemies which resulted in some nice sequences where the agent avoided enemies successfully:</p>
								<div class="video-container">
									<video controls autoplay>
										<source src="../videos/DqnGameResult7.mp4" type="video/mp4">
										Your browser does not support the video tag.
									</video>
									<br><br>
								</div>

								<p>The agent was far from perfect but the PoC showcased how DQN can be used to train an agent in a dynamic environment. It also showed how important a meaningful rewarads structure is and how the rewards affect the agent's behavior.</p>

								<p><strong>Conclusion</strong></p>
								<p>Building a Reinforcement Learning (RL) agent requires a deep understanding of the training process, particularly the reward function, input representation, and tuning of training parameters. The complexity arises from the interdependence of various variables, making trial and error a crucial part of optimizing the agent's performance. Increasing the number of training episodes significantly prolonged training sessions, with each run taking approximately 2-3 hours, making further refinements more challenging.</p>
							</div>
						</div>						
					</div>
				</div>

			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>
			<script src="../assets/js/darkmode.js"></script>

	</body>
</html>